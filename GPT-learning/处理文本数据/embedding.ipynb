{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26748aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef642ba",
   "metadata": {},
   "source": [
    "#### Embedding（将token id嵌入）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea15083",
   "metadata": {},
   "source": [
    "#### 嵌入层只是独热编码和矩阵乘法方法的一种更高效的实现，因此它可以被视为一个能够通过反向传播进行优化的神经网络层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcfe56",
   "metadata": {},
   "source": [
    "#### 绝对位置嵌入：词向量 + 位置向量\n",
    "#### 相对位置嵌入：加入依赖位置差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374ab659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        super().__init__()\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1:i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7881da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "# 构建embedding层\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# DataLoader\n",
    "def GPTDataLoader(txt, batch_size, max_length, stride, shuffle = True, \n",
    "                  drop_last = True, num_workers = 0, tokenizer = tokenizer):\n",
    "    dataset = GPTDataset(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return dataloader\n",
    "with open(\"the-verdict.txt\", \"r\", encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = GPTDataLoader(raw_text, batch_size=8, max_length=4,\n",
    "stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, target = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs) \n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ae27c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs) \n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fd2f5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# 得到绝对位置编码\n",
    "context_length = 4\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length)) \n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81ad81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最终得到的embedding向量\n",
    "input_embedding = token_embeddings + pos_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc45ca",
   "metadata": {},
   "source": [
    "### One-Hot编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64edf9fc",
   "metadata": {},
   "source": [
    "#### one-hot编码，用一个向量表示一个物体，Embedding就是one-hot编码的压缩变换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b48b6",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28603763",
   "metadata": {},
   "source": [
    "#### Word2Vec是训练Embedding 的一种方法\n",
    "#### 直接吧词转换为向量\n",
    "#### 包括CBOW和Skip-Gram两种方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d73ee",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8353e28",
   "metadata": {},
   "source": [
    "#### 与Word2Vec类似，都是根据上下文预测，但是每个词由多个子词向量相加而成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe73f70",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1570ed65",
   "metadata": {},
   "source": [
    "### 上面两种是居于局部上下文进行预测，Glove是居于全局的统计信息预测\n",
    "### 🧩 1. 构建共现矩阵\n",
    "\n",
    "在整个语料库中，统计每个词与上下文词出现的次数：\n",
    "\n",
    "- \\( X_{ij} \\)：单词 \\( j \\) 在单词 \\( i \\) 的上下文中出现的次数  \n",
    "- \\( X_i = \\sum_k X_{ik} \\)：单词 \\( i \\) 的上下文总出现次数  \n",
    "\n",
    "于是可以得到共现概率：\n",
    "\n",
    "\\[\n",
    "P_{ij} = \\frac{X_{ij}}{X_i}\n",
    "\\]\n",
    "\n",
    "表示「在词 \\( i \\) 的上下文中看到词 \\( j \\)」的概率。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 2. 共现比值反映语义关系\n",
    "\n",
    "若两个词 \\( i, j \\) 与一个上下文词 \\( k \\) 的共现概率相似，说明它们语义接近。\n",
    "\n",
    "例如：\n",
    "\n",
    "- “ice” 与 “solid” 共现较高  \n",
    "- “steam” 与 “gas” 共现较高  \n",
    "- “ice” 与 “gas” 共现较低  \n",
    "\n",
    "这种语义关系可以用 **共现概率比值** 表达：\n",
    "\n",
    "\\[\n",
    "\\frac{P_{ik}}{P_{jk}}\n",
    "\\]\n",
    "\n",
    "这个比值捕捉了「词与词之间的语义差异」。\n",
    "\n",
    "---\n",
    "\n",
    "### ⚖️ 3. 模型目标\n",
    "\n",
    "我们希望找到词向量 \\( w_i \\) 和上下文向量 \\( \\tilde{w}_j \\)，  \n",
    "使得它们的关系能近似地表示这个概率比值：\n",
    "\n",
    "\\[\n",
    "f(w_i, w_j, \\tilde{w}_k) = \\frac{P_{ik}}{P_{jk}}\n",
    "\\]\n",
    "\n",
    "通过一系列推导，GloVe 得出一个更简单的线性方程：\n",
    "\n",
    "\\[\n",
    "w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j = \\log(X_{ij})\n",
    "\\]\n",
    "\n",
    "即：\n",
    "\n",
    "> 两个词向量的点积 + 偏置 ≈ 它们共现次数的对数\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 4. 最终优化目标（损失函数）\n",
    "\n",
    "GloVe 使用加权平方误差最小化整个语料的误差：\n",
    "\n",
    "\\[\n",
    "J = \\sum_{i,j=1}^{V} f(X_{ij}) \\, (w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij})^2\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "\n",
    "- \\( f(X_{ij}) \\) 是权重函数，抑制极端频率的影响：\n",
    "  \\[\n",
    "  f(x) = \n",
    "  \\begin{cases}\n",
    "  (x / x_{\\text{max}})^{\\alpha}, & \\text{if } x < x_{\\text{max}} \\\\\n",
    "  1, & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  \\]\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 5. 向量语义示例\n",
    "\n",
    "GloVe 学得的词向量具有语义运算特性：\n",
    "\n",
    "\\[\n",
    "\\text{vec}(\"king\") - \\text{vec}(\"man\") + \\text{vec}(\"woman\") \\approx \\text{vec}(\"queen\")\n",
    "\\]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
