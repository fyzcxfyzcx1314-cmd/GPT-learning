{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f2a5ae6",
   "metadata": {},
   "source": [
    "#### Transform的核心是self-attention，但自注意力是无须的，它只看token之间的相关性\n",
    "#### 所以我们通过位置编码，为序列中的每个token添加位置信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c973b",
   "metadata": {},
   "source": [
    "### 一. 正弦编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c63b58b",
   "metadata": {},
   "source": [
    "$$\n",
    "PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046fed0c",
   "metadata": {},
   "source": [
    "pos = token 在序列中的位置（0, 1, 2, …）\n",
    "\n",
    "i = 维度索引\n",
    "\n",
    "d_model = 词向量维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecab1fba",
   "metadata": {},
   "source": [
    "得到pos_emb直接与token_emb相加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a81a3",
   "metadata": {},
   "source": [
    "### 二. 可学习绝对位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f05b6",
   "metadata": {},
   "source": [
    "定义一个可学习的与token_emb类似的矩阵P\n",
    "每个位置对应P[pos]\n",
    "然后与token_emb相加"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
