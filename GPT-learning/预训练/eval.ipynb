{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73694636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from GPT import GPT\n",
    "import tiktoken\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a18b822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (token_embed): Embedding(50257, 768)\n",
       "  (pos_embed): Embedding(256, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (block): Sequential(\n",
       "    (0): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (wq): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wk): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (wv): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (LayerNorm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LayerNorm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = { \n",
    " \"vocab_size\": 50257, \n",
    " \"context_length\": 256, \n",
    " \"emb_dim\": 768, \n",
    " \"n_heads\": 12, \n",
    " \"n_layers\": 12, \n",
    " \"drop_rate\": 0.1, \n",
    " \"qkv_bias\": False \n",
    "}\n",
    "torch.manual_seed(123) \n",
    "model = GPT(GPT_CONFIG_124M) \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50953934",
   "metadata": {},
   "source": [
    "### GPT生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11ea9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_text, context_size):\n",
    "    for _ in range(max_text):\n",
    "        idx = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx)\n",
    "        logits = logits[:, -1, :]\n",
    "        idx_text = torch.softmax(logits, dim = -1)\n",
    "        idx_next = torch.argmax(idx_text, dim = -1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07a6472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_id(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdc46e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_text(id, tokenizer):\n",
    "    flat = id.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c01a62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexAngel infieldcigans\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\" \n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple( \n",
    " model=model, \n",
    " idx=text_to_id(start_context, tokenizer), \n",
    " max_text=10, \n",
    " context_size=GPT_CONFIG_124M[\"context_length\"] \n",
    ") \n",
    "print(\"Output text:\\n\", id_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540640a",
   "metadata": {},
   "source": [
    "#### 计算文本生成损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558c2d74",
   "metadata": {},
   "source": [
    "#### 使用交叉熵随时，来衡量两个概率分布之间的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93143f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cross_entropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff179b",
   "metadata": {},
   "source": [
    "### 计算训练集和验证集的损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230dc701",
   "metadata": {},
   "source": [
    "#### 单个批次的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76e95376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(inputs, targets, model, device):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "    logits = model(inputs)\n",
    "    loss = F.cross_entropy(logits.flatten(0, 1), targets.flatten())\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6108c6",
   "metadata": {},
   "source": [
    "#### 数据加载器采样的所有批次的损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "604198b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clac_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(inputs, targets, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
