{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7ddb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "torch version: 2.9.0\n",
      "transformers version: 4.57.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"numpy\", \"torch\", \"transformers\"]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9efaa6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e67fdc2e694b6ea9ac6e450e4420a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29611\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\works\\GPT-learning\\加载预训练权重\\checkpoints\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61b8a4ef0d54f30b4b31088bf78f259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2Attention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "\n",
    "\n",
    "# allowed model names\n",
    "model_names = {\n",
    "    \"gpt2-small (124M)\": \"openai-community/gpt2\",\n",
    "    \"gpt2-medium (355M)\": \"openai-community/gpt2-medium\",\n",
    "    \"gpt2-large (774M)\": \"openai-community/gpt2-large\",\n",
    "    \"gpt2-xl (1558M)\": \"openai-community/gpt2-xl\"\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "\n",
    "gpt_hf = GPT2Model.from_pretrained(model_names[CHOOSE_MODEL], cache_dir=\"checkpoints\")\n",
    "gpt_hf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ed0f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": True        # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0268685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_check(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(right.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ceef97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_weights(gpt, gpt_hf):\n",
    "\n",
    "    d = gpt_hf.state_dict()\n",
    "\n",
    "    gpt.pos_embed.weight = assign_check(gpt.pos_embed.weight, d[\"wpe.weight\"])\n",
    "    gpt.token_embed.weight = assign_check(gpt.token_embed.weight, d[\"wte.weight\"])\n",
    "    \n",
    "    for b in range(BASE_CONFIG[\"n_layers\"]):\n",
    "        q_w, k_w, v_w = np.split(d[f\"h.{b}.attn.c_attn.weight\"], 3, axis=-1)\n",
    "        gpt.block[b].att.wq.weight = assign_check(gpt.block[b].att.wq.weight, q_w.T)\n",
    "        gpt.block[b].att.wk.weight = assign_check(gpt.block[b].att.wk.weight, k_w.T)\n",
    "        gpt.block[b].att.wv.weight = assign_check(gpt.block[b].att.wv.weight, v_w.T)\n",
    "    \n",
    "        q_b, k_b, v_b = np.split(d[f\"h.{b}.attn.c_attn.bias\"], 3, axis=-1)\n",
    "        gpt.block[b].att.wq.bias = assign_check(gpt.block[b].att.wq.bias, q_b)\n",
    "        gpt.block[b].att.wk.bias = assign_check(gpt.block[b].att.wk.bias, k_b)\n",
    "        gpt.block[b].att.wv.bias = assign_check(gpt.block[b].att.wv.bias, v_b)\n",
    "    \n",
    "    \n",
    "        gpt.block[b].att.out_proj.weight = assign_check(gpt.block[b].att.out_proj.weight, d[f\"h.{b}.attn.c_proj.weight\"].T)\n",
    "        gpt.block[b].att.out_proj.bias = assign_check(gpt.block[b].att.out_proj.bias, d[f\"h.{b}.attn.c_proj.bias\"])\n",
    "    \n",
    "        gpt.block[b].ffn[0].weight = assign_check(gpt.block[b].ffn[0].weight, d[f\"h.{b}.mlp.c_fc.weight\"].T)\n",
    "        gpt.block[b].ffn[0].bias = assign_check(gpt.block[b].ffn[0].bias, d[f\"h.{b}.mlp.c_fc.bias\"])\n",
    "        gpt.block[b].ffn[2].weight = assign_check(gpt.block[b].ffn[2].weight, d[f\"h.{b}.mlp.c_proj.weight\"].T)\n",
    "        gpt.block[b].ffn[2].bias = assign_check(gpt.block[b].ffn[2].bias, d[f\"h.{b}.mlp.c_proj.bias\"])\n",
    "    \n",
    "        gpt.block[b].LayerNorm1.weight = assign_check(gpt.block[b].LayerNorm1.weight, d[f\"h.{b}.ln_1.weight\"])\n",
    "        gpt.block[b].LayerNorm1.bias = assign_check(gpt.block[b].LayerNorm1.bias, d[f\"h.{b}.ln_1.bias\"])\n",
    "        gpt.block[b].LayerNorm2.weight = assign_check(gpt.block[b].LayerNorm2.weight, d[f\"h.{b}.ln_2.weight\"])\n",
    "        gpt.block[b].LayerNorm2.bias = assign_check(gpt.block[b].LayerNorm2.bias, d[f\"h.{b}.ln_2.bias\"])\n",
    "    \n",
    "        gpt.norm.weight = assign_check(gpt.norm.weight, d[\"ln_f.weight\"])\n",
    "        gpt.norm.bias = assign_check(gpt.norm.bias, d[\"ln_f.bias\"])\n",
    "        gpt.out_head.weight = assign_check(gpt.out_head.weight, d[\"wte.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "94b8fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from GPT模型架构 import GPT\n",
    "# For llms_from_scratch installation instructions, see:\n",
    "# https://github.com/rasbt/LLMs-from-scratch/tree/main/\n",
    "\n",
    "\n",
    "gpt = GPT(BASE_CONFIG)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_weights(gpt, gpt_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6dabfb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, \n",
    "             temperature=0.0, top_k=None, eos_id=None): \n",
    "   for _ in range(max_new_tokens): \n",
    "      idx_cond = idx[:, -context_size:] \n",
    "      with torch.no_grad(): \n",
    "         logits = model(idx_cond) \n",
    "      logits = logits[:, -1, :] \n",
    "      if top_k is not None: \n",
    "         top_logits, _ = torch.topk(logits, top_k)\n",
    "         min_val = top_logits[:, -1] \n",
    "         logits = torch.where( \n",
    "            logits < min_val, \n",
    "            torch.tensor(float('-inf')).to(logits.device), \n",
    "            logits \n",
    "            ) \n",
    "         if temperature > 0.0: \n",
    "            logits = logits / temperature \n",
    "            probs = torch.softmax(logits, dim=-1) \n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "         else: \n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True) \n",
    "         if idx_next == eos_id: \n",
    "            break \n",
    "         idx = torch.cat((idx, idx_next), dim=1) \n",
    "   return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4894aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_id(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def id_to_text(id, tokenizer):\n",
    "    flat = id.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "22b2e9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt.eval()\n",
    "token_ids = generate(\n",
    "    model=gpt.to(device),\n",
    "    idx=text_to_id(\"Every effort moves\", tokenizer).to(device),\n",
    "    max_new_tokens=30,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    top_k=1,\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", id_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
