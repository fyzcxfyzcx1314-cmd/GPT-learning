{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a8fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "from functools import partial\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from torch.nn import functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970961f",
   "metadata": {},
   "source": [
    "### 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1e278887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "def download_and_load_file(file_path, url):\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as respons:\n",
    "            text_data = respons.read().decode(\"utf-8\")\n",
    "        with open (file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "file_path = \"../dataset/instruction-data.json\" \n",
    "url = ( \n",
    " \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\" \n",
    " \"/main/ch07/01_main-chapter-code/instruction-data.json\" \n",
    ") \n",
    "data = download_and_load_file(file_path, url) \n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "df6adbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': \"Provide a synonym for 'kind'.\", 'input': '', 'output': \"A synonym for 'kind' is 'benevolent'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example entry:\\n\", data[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91125bc4",
   "metadata": {},
   "source": [
    "### 提示词风格"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec985a8e",
   "metadata": {},
   "source": [
    "#### 将指令集中的数据转换为两种提示词风格，Alpac和Phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75850bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\" \n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79660b",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7786550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集: 935\n",
      "验证集: 55\n",
      "测试集: 110\n"
     ]
    }
   ],
   "source": [
    "train_data, temp_data = train_test_split(data, train_size=0.85, random_state=42)\n",
    "test_data, val_data = train_test_split(temp_data, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"训练集: {len(train_data)}\")\n",
    "print(f\"验证集: {len(val_data)}\")\n",
    "print(f\"测试集: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6adc77b",
   "metadata": {},
   "source": [
    "#### dataset与dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fe556c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_text = []\n",
    "        for entry in data:\n",
    "            instruction_inputs = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_inputs + response_text\n",
    "            self.encoded_text.append(tokenizer.encode(full_text))\n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_text[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8d30f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, pad_token=50256, ignore_token=-100, allowed_max_length=None,device=\"cpu\"):\n",
    "    batch_max_lemgth = max(len(item) + 1 for item in batch)\n",
    "    input_lst, target_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token]\n",
    "        padded = (new_item + [pad_token] * (batch_max_lemgth - len(new_item)))\n",
    "        inputs = padded[:-1]\n",
    "        targets = padded[1:]\n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long, device=device)\n",
    "        targets = torch.tensor(targets, dtype=torch.long, device=device)\n",
    "        mask = targets == pad_token\n",
    "        indices = torch.nonzero(mask).squeeze() \n",
    "        if indices.numel() > 1: \n",
    "            targets[indices[1:]] = ignore_token\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        input_lst.append(inputs)\n",
    "        target_lst.append(targets)\n",
    "    \n",
    "    input_tensor = torch.stack(input_lst).to(device)\n",
    "    target_tensor = torch.stack(target_lst).to(device)\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2afe5125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]]), tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]]))\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4] \n",
    "inputs_2 = [5, 6] \n",
    "inputs_3 = [7, 8, 9] \n",
    "batch = ( \n",
    " inputs_1, \n",
    " inputs_2, \n",
    " inputs_3 \n",
    ") \n",
    "print(custom_collate_fn(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fbf23cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "508f7436",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_works = 0\n",
    "batch_size = 8\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_works\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_works\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_works\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd05f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1c65abda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite this statement as an imperative sentence.\n",
      "\n",
      "### Input:\n",
      "You should finish your assignment.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123) \n",
    "input_text = format_input(val_data[0]) \n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2130fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Output:\n",
      "\n",
      "You should complete your assignment.\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "inputs_id = tokenizer.encode(input_text)\n",
    "inputs_tensor = torch.tensor(inputs_id).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        inputs_tensor,\n",
    "        max_length = 60,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "generated_ids = output[0].tolist()\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "\n",
    "num_input_ids = len(inputs_id)\n",
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80f731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(inputs, targets, model, device):\n",
    "    inputs = inputs.to(device)\n",
    "    targets = targets.to(device)\n",
    "\n",
    "    model.train() \n",
    "    outputs = model(inputs, labels=targets)\n",
    "\n",
    "    # 获取损失\n",
    "    loss = outputs.loss\n",
    "    return loss  # 返回张量\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.0\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(inputs, targets, model, device)\n",
    "            total_loss += loss\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "170caa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: tensor(262.3800)\n",
      "Validation loss: tensor(274.9545)\n"
     ]
    }
   ],
   "source": [
    "model.to(device) \n",
    "torch.manual_seed(123) \n",
    "with torch.no_grad(): \n",
    "    train_loss = calc_loss_loader( \n",
    "        train_loader, model, device,num_batches=5\n",
    "        ) \n",
    "    val_loss = calc_loss_loader( \n",
    "        val_loader, model, device, num_batches=5\n",
    "        ) \n",
    "print(\"Training loss:\", train_loss) \n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "06b1457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, \n",
    "                                      num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, \n",
    "                                      num_batches=eval_iter)\n",
    "        model.train()\n",
    "        return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "327eb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_text, context_size):\n",
    "    for _ in range(max_text):\n",
    "        idx = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx)\n",
    "        logits = logits[:, -1, :]\n",
    "        idx_text = torch.softmax(logits, dim = -1)\n",
    "        idx_next = torch.argmax(idx_text, dim = -1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2330961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_embed.weight.shape[0]\n",
    "\n",
    "    inputs_id = tokenizer.encode(start_context)\n",
    "    encoded = torch.tensor(inputs_id).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model = model, idx = encoded,\n",
    "            max_text = 50, context_size = context_size\n",
    "        )\n",
    "\n",
    "    generated_id = output[0].tolist()\n",
    "    decoded_text = tokenizer.decode(generated_id)\n",
    "    print(decoded_text.replace(\"\\n\", \" \")) \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ea7b7414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.config.n_positions\n",
    "\n",
    "    inputs_id = tokenizer.encode(start_context)\n",
    "    inputs_tensor = torch.tensor(inputs_id).unsqueeze(0).to(device)\n",
    "    if inputs_tensor.shape[1] > context_size:\n",
    "        inputs_tensor = inputs_tensor[:, :-context_size]\n",
    "\n",
    "    max_length = context_size + 50\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(inputs_tensor, max_length=max_length, num_return_sequences=1)\n",
    "    generated_id = output[0].tolist()\n",
    "    generate_text = tokenizer.decode(generated_id)\n",
    "    \n",
    "    response_text = generate_text[len(input_text):].strip()\n",
    "    print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cbb85290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, device, \n",
    "          num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_token_seen = [], [], []\n",
    "    token_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(inputs, targets, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            token_seen += inputs.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                    )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_token_seen.append(token_seen)\n",
    "\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \" \n",
    "                      f\"Train loss {train_loss:.3f}, \" \n",
    "                      f\"Val loss {val_loss:.3f}\" \n",
    "                      )\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_token_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "07d97c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 290.605, Val loss 291.868\n",
      "Ep 1 (Step 000005): Train loss 293.196, Val loss 291.232\n",
      "Ep 1 (Step 000010): Train loss 295.004, Val loss 290.369\n",
      "Ep 1 (Step 000015): Train loss 291.270, Val loss 289.461\n",
      "Ep 1 (Step 000020): Train loss 285.863, Val loss 288.645\n",
      "Ep 1 (Step 000025): Train loss 289.004, Val loss 287.579\n",
      "Ep 1 (Step 000030): Train loss 288.944, Val loss 286.971\n",
      "Ep 1 (Step 000035): Train loss 286.676, Val loss 286.135\n",
      "Ep 1 (Step 000040): Train loss 285.916, Val loss 285.258\n",
      "Ep 1 (Step 000045): Train loss 289.763, Val loss 284.535\n",
      "Ep 1 (Step 000050): Train loss 275.644, Val loss 283.787\n",
      "Ep 1 (Step 000055): Train loss 295.062, Val loss 282.968\n",
      "Ep 1 (Step 000060): Train loss 282.692, Val loss 282.200\n",
      "Ep 1 (Step 000065): Train loss 279.132, Val loss 280.997\n",
      "Ep 1 (Step 000070): Train loss 287.079, Val loss 280.205\n",
      "Ep 1 (Step 000075): Train loss 282.901, Val loss 279.452\n",
      "Ep 1 (Step 000080): Train loss 285.635, Val loss 278.762\n",
      "Ep 1 (Step 000085): Train loss 280.825, Val loss 277.944\n",
      "Ep 1 (Step 000090): Train loss 272.230, Val loss 276.858\n",
      "Ep 1 (Step 000095): Train loss 280.431, Val loss 276.441\n",
      "Ep 1 (Step 000100): Train loss 283.215, Val loss 275.435\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00005\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      4\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \n\u001b[1;32m----> 6\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m      7\u001b[0m     model, train_loader, val_loader, optimizer, device, \n\u001b[0;32m      8\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, eval_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[0;32m      9\u001b[0m     start_context\u001b[38;5;241m=\u001b[39mformat_input(val_data[\u001b[38;5;241m0\u001b[39m]), tokenizer\u001b[38;5;241m=\u001b[39mtokenizer) \n\u001b[0;32m     11\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \n\u001b[0;32m     12\u001b[0m execution_time_minutes \u001b[38;5;241m=\u001b[39m (end_time \u001b[38;5;241m-\u001b[39m start_time) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m \n",
      "Cell \u001b[1;32mIn[105], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m calc_loss_batch(inputs, targets, model, device)\n\u001b[1;32m---> 12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m token_seen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[1;32mc:\\Users\\29611\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    624\u001b[0m     )\n\u001b[1;32m--> 625\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    627\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\29611\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m _engine_run_backward(\n\u001b[0;32m    355\u001b[0m     tensors,\n\u001b[0;32m    356\u001b[0m     grad_tensors_,\n\u001b[0;32m    357\u001b[0m     retain_graph,\n\u001b[0;32m    358\u001b[0m     create_graph,\n\u001b[0;32m    359\u001b[0m     inputs_tuple,\n\u001b[0;32m    360\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    362\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\29611\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    842\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    843\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time() \n",
    "torch.manual_seed(123) \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "num_epochs = 2 \n",
    "\n",
    "train_losses, val_losses, tokens_seen = train(\n",
    "    model, train_loader, val_loader, optimizer, device, \n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5, \n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer) \n",
    "\n",
    "end_time = time.time() \n",
    "execution_time_minutes = (end_time - start_time) / 60 \n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609a4421",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'example_seen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     17\u001b[0m epochs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, num_epochs, \u001b[38;5;28mlen\u001b[39m(train_losses)) \n\u001b[1;32m---> 18\u001b[0m examples_seen_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, example_seen, \u001b[38;5;28mlen\u001b[39m(train_losses)) \n\u001b[0;32m     19\u001b[0m plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'example_seen' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, \n",
    "    label=\"loss\"): \n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\") \n",
    "    ax1.plot( epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\") \n",
    "    ax1.set_xlabel(\"Epochs\") \n",
    "    ax1.set_ylabel(label.capitalize()) \n",
    "    ax1.legend() \n",
    "    ax2 = ax1.twiny() \n",
    "    ax2.plot(examples_seen, train_values, alpha=0) \n",
    "    ax2.set_xlabel(\"Examples seen\") \n",
    "    fig.tight_layout() \n",
    "    plt.savefig(f\"{label}-plot.pdf\") \n",
    "    plt.show()\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses)) \n",
    "examples_seen_tensor = torch.linspace(0, example_seen, len(train_losses)) \n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5671d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, max_new_length):\n",
    "    model.eval()\n",
    "    inputs_id = tokenizer.encode(input_text)\n",
    "    inputs_tensor = torch.tensor(inputs_id).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            inputs_tensor,\n",
    "            max_length = 60,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "    generated_ids = output[0].tolist()\n",
    "    generated_text = tokenizer.decode(generated_ids)\n",
    "\n",
    "    response_text = generated_text[len(input_text):].strip()\n",
    "    return response_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
