{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ac38ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0f029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CONFIG = { \n",
    " \"vocab_size\": 50257, \n",
    " \"context_length\": 1024, \n",
    " \"drop_rate\": 0.0, \n",
    " \"qkv_bias\": True \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9904916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49c3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, csv_file, tokenizer, max_length, pad_id=50256):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.encoded_text = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "        self.encoded_text = [\n",
    "            encoded_text[:self.max_length] for encoded_text in self.encoded_text\n",
    "        ]\n",
    "        self.encoded_text = [\n",
    "            encoded_text + [pad_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_text\n",
    "        ]\n",
    "    def _longest_encoded_length(self):\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_text:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "    def __getitem__(self, index):\n",
    "        encoded = self.encoded_text[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),\n",
    "            torch.tensor(label, dtype=torch.long)\n",
    "        )\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "114bca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SpamDataset(\n",
    "    csv_file=\"../dataset/train.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=None\n",
    ")\n",
    "val_dataset = SpamDataset(\n",
    "    csv_file=\"../dataset/test.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length,\n",
    ")\n",
    "test_dataset = SpamDataset(\n",
    "    csv_file=\"../dataset/test.csv\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=train_dataset.max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7d5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d9a762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch dimensions: torch.Size([8, 104])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in train_loader: \n",
    " pass \n",
    "print(\"Input batch dimensions:\", input_batch.shape) \n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd3c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "38 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\") \n",
    "print(f\"{len(val_loader)} validation batches\") \n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83a679c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_id(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def id_to_text(id, tokenizer):\n",
    "    flat = id.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "def generate_text_simple(model, idx, max_text, context_size):\n",
    "    for _ in range(max_text):\n",
    "        idx = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx)\n",
    "        logits = logits[:, -1, :]\n",
    "        idx_text = torch.softmax(logits, dim = -1)\n",
    "        idx_next = torch.argmax(idx_text, dim = -1, keepdim=True)\n",
    "        idx = torch.cat((idx, idx_next), dim = -1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30edc49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2fead3b177406fb7f940c38eeed9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29611\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\29611\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ef401e9f1f4a85ae4a75e88f4aec85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e34559a05f470a9b60ea60bf3f13f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7802570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work.\n",
      "\n",
      "The second step is to understand the importance of your work.\n",
      "\n",
      "The third step is to understand the importance of your work.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "text1 = \"Every effort moves you\"\n",
    "inputs_id = tokenizer.encode(text1)\n",
    "input_tensor = torch.tensor(inputs_id).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tensor, max_length = 50, num_return_sequences=1)\n",
    "generate_ids = output[0].tolist()\n",
    "generate_text1 = tokenizer.decode(generate_ids)\n",
    "print(generate_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02dfd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe1e1ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4347e30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head = nn.Linear(in_features=768, out_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75d7b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_blocks = model.transformer.h\n",
    "last_block_index = len(transformer_blocks) - 1\n",
    "for param in transformer_blocks[last_block_index].parameters():\n",
    "    param.requires_grad = True\n",
    "model.transformer.ln_f.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b6d1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可训练参数: transformer.h.11.ln_1.weight\n",
      "可训练参数: transformer.h.11.ln_1.bias\n",
      "可训练参数: transformer.h.11.attn.c_attn.weight\n",
      "可训练参数: transformer.h.11.attn.c_attn.bias\n",
      "可训练参数: transformer.h.11.attn.c_proj.weight\n",
      "可训练参数: transformer.h.11.attn.c_proj.bias\n",
      "可训练参数: transformer.h.11.ln_2.weight\n",
      "可训练参数: transformer.h.11.ln_2.bias\n",
      "可训练参数: transformer.h.11.mlp.c_fc.weight\n",
      "可训练参数: transformer.h.11.mlp.c_fc.bias\n",
      "可训练参数: transformer.h.11.mlp.c_proj.weight\n",
      "可训练参数: transformer.h.11.mlp.c_proj.bias\n",
      "可训练参数: lm_head.weight\n",
      "可训练参数: lm_head.bias\n"
     ]
    }
   ],
   "source": [
    "# 输出确认状态\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"可训练参数: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b615bc62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
